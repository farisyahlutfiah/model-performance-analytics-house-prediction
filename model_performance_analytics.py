# -*- coding: utf-8 -*-
"""#DS5H17_Farisyah Lutfiah Hanis_Model Performance Analytics

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19rIBHeEa9ugTgpWumlQXqc52Wl2i3Y9F

#####NAMA : FARISYAH LUTFIAH HANIS
#####ID   : DS5H17

#IMPORT LIBRARY & DATASET
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
from datetime import datetime
from sklearn.preprocessing import LabelEncoder

from scipy import stats
from scipy.stats import norm

from google.colab import drive
drive.mount('/content/drive')

"""#DATA UNDERSTANDING"""

df = pd.read_csv('/content/drive/My Drive/Dataset/house_price/train.csv')
pd.set_option('display.max_columns', 100) #agar seluruh fitur dapat terlihat
df.head(10)

df.info()

"""- Fitur berjumlah 81, dengan tipe data yaitu integer, float, dan char
- Total baris berjumlah 1460
- Fitur yang mengandung missing value yaitu Alley, MasVnrType, MasVnrArea, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, Electrical, FireplaceQu, GarageType, GarageYrBlt, GarageFinish, GarageQual, GarageCond, PoolQC, Fence, MiscFeature.
"""

# untuk melihat jumlah baris dan kolom
df.shape

pd.set_option('display.max_columns', 100)
df.describe()

"""Harga rumah ini dari range 34900 sampai 755000 dengan rata-rata harga rumah 163000. Sebaran harga rumahnya yaitu right skew, terlihat dari nilai mean lebih besar mediannya, artinya data cenderung lebih banyak berkumpul pada nilai minimum."""

pd.set_option('display.max_columns', 43)
df.describe(include=object) #khusus tipe data object

numeric_data = df.select_dtypes(include=[np.number])
cat_data = df.select_dtypes (exclude = [np.number])
(numeric_data.shape[1], cat_data.shape[1])

sns.set(font_scale=1.1)
corr_df = df.corr()
mask = np.triu(corr_df.corr())
plt.figure(figsize=(20, 20))
sns.heatmap(corr_df, annot=True, fmt='.1f', cmap='coolwarm', square=True, mask=mask, linewidth=1, cbar=True)
plt.show()

# karena hasil korelasi tidak dilihat karena banyaknya fitur, maka akan coba menampilkan informasi dari 15 korelasi tertinngi
print(corr_df['SalePrice'].sort_values(ascending=False)[:15],'\n')

"""#DATA PREPARATION

##1. Missing Value

###Check Missing Value
"""

df.isnull().sum()

missing_values = df.isnull().sum()
print(missing_values[missing_values > 0])

"""Ini merupakan daftar fitur fitur yang memiliki missing value"""

missing_values = df.isnull().sum()/len(df)*100
print(missing_values[missing_values > 0])

"""Waw, ternyata ada fitur yang missing valuenya hampir 100%üòØ

###Handling Missing Value
"""

#Dropping missing value
df = df.drop(['Alley','Fence','PoolQC','MiscFeature'], axis=1)
df.head()

"""Fitur fitur tersebut dihapus karena memiliki missing value lebih dari 80%"""

# cek kembali jumlah baris dan kolom
df.shape

#Imputing missing values LotFrontage
df['LotFrontage'].fillna(df['LotFrontage'].mean(), inplace=True)

#Imputing missing values LotFrontage
df['MasVnrArea'].fillna(df['MasVnrArea'].median(), inplace=True)

#Imputing missing values GarageYrBlt
df['GarageYrBlt'].fillna(df['GarageYrBlt'].median(), inplace=True)

#Menghitung nilai modus untuk semua kolom kategorikal
mode_values = df.select_dtypes(include='object').mode().iloc[0]

#Mengisi missing value di kolom kategorikal dengan nilai modus
df.fillna(mode_values, inplace=True)

missing_values = df.isnull().sum()
print(missing_values[missing_values > 0])

"""Missing value sudah tidak ada lagiüëç"""

df.isnull().sum()

"""##2. Duplicates

###Check Duplicate
"""

# Check duplicate values
df[df.duplicated(keep=False)]

"""Ternyata tidak ada nilai yang duplikatüòÄ

##3. Outliers

###SalePrice
"""

fig, ax = plt.subplots(figsize=(3,6))
sns.boxplot(data=df['SalePrice'])

"""Pada fitur SalePrice terdapat nilai outlier diluar batas nilai maksimum"""

#Tentukan batas bawah dan batas atas untuk outlier
Q1 = df['SalePrice'].quantile(0.25)
Q3 = df['SalePrice'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outlier_value = df[(df['SalePrice'] < lower_bound) | (df['SalePrice'] > upper_bound)]
outlier_value

"""Nilai outliersnya adalah nilai yang lebih dari upper_boundnya"""

#hapus nilai yang outlier
df = df[(df['SalePrice'] >= lower_bound) & (df['SalePrice'] <= upper_bound)]

#cek kembali baris dan kolom
df.shape

fig, ax = plt.subplots(figsize=(3,6))
sns.boxplot(data=df['SalePrice'])

"""###YearBuilt"""

fig, ax = plt.subplots(figsize=(3,6))
sns.boxplot(data=df['YearBuilt'])

"""Pada fitur YearBuilt terdapat nilai outlier diluar batas nilai minimum

IMPUTATION
"""

#Hitung nilai median dari atribut YearBuilt
median_year_built = df['YearBuilt'].median()

#Tentukan batas bawah dan batas atas untuk outlier
Q1 = df['YearBuilt'].quantile(0.25)
Q3 = df['YearBuilt'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

#Imputasi outlier dengan nilai median
data_imputasi = df.loc[(df['YearBuilt'] < lower_bound) | (df['YearBuilt'] > upper_bound), 'YearBuilt'] = median_year_built

"""### GrLivArea"""

fig, ax = plt.subplots(figsize=(3,6))
sns.boxplot(data=df['GrLivArea'])

"""Pada fitur GrLivArea terdapat nilai outlier diluar batas nilai maksimum

IMPUTATION
"""

#Hitung nilai median dari atribut YearBuilt
mean_grade_living = df['GrLivArea'].mean()

#Tentukan batas bawah dan batas atas untuk outlier
Q1 = df['GrLivArea'].quantile(0.25)
Q3 = df['GrLivArea'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

#Imputasi outlier dengan nilai mean
df.loc[(df['GrLivArea'] < lower_bound) | (df['GrLivArea'] > upper_bound), 'GrLivArea'] = mean_grade_living

"""###LotArea"""

fig, ax = plt.subplots(figsize=(3,6))
sns.boxplot(data=df['LotArea'])

"""Pada fitur LotArea terdapat nilai outlier diluar batas nilai minimum dan maksimum

IMPUTATION
"""

#Hitung nilai mean dari atribut YearBuilt
mean_grade_living = df['LotArea'].mean()

#Tentukan batas bawah dan batas atas untuk outlier
Q1 = df['LotArea'].quantile(0.25)
Q3 = df['LotArea'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

#Imputasi outlier dengan nilai mean
df.loc[(df['LotArea'] < lower_bound) | (df['LotArea'] > upper_bound), 'LotArea'] = mean_grade_living

print(corr_df['SalePrice'].sort_values(ascending=False)[:15],'\n')

"""# FEATURE SELECTION

Pemilihan fitur yang akan dimasukkan ke dalam model. Ada 10 fitur yang akan dimasukkan ke dalam model regresi karena fitur ini yang memiliki hubungan yang kuat dengan variabel target. Fitur-fitur tersebut adalah OverallQual (0.79), GrLivArea (0.70), GarageCars (0.64), GarageArea (0.62), TotalBsmtSF (0.61), 1stFlrSF (0.60), FullBath (0.56), TotRmsAbvGrd (0.53), YearBuilt (0.52), dan YearRemodAdd (0.50).

#MODELLING
"""

#pisahkan varible X dan Y, dimana X diisi oleh variable yang dibutuhkan aja yang udah kita tentukan di feature selection
x = df[['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF',
        '1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt', 'YearRemodAdd']]

y = pd.DataFrame(df['SalePrice'])

#membagai dataset menjadi data training dan data testing dengan proporsi 80:20
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42, test_size=.33)

x

y

# 80% data untuk dilatih
x_train

"""##Feature Scalling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.fit_transform(x_test)

""" Penskalaan data pada data latih (x_train) dan data uji (x_test) menggunakan StandardScaler"""

#menampilkan dataset X_train setelah di scaling
np.set_printoptions(suppress=True)
print(x_train[:10])

print(y_train[:10])

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators = 100,
                           n_jobs = -1,
                           oob_score = True,
                           bootstrap = True,
                           random_state = 42)
rf.fit(x_train, y_train)

# with function -----

# function for creating a feature importance dataframe
def imp_df(column_names, importances):
    df = pd.DataFrame({'feature': column_names,
                       'feature_importance': importances}) \
           .sort_values('feature_importance', ascending = False) \
           .reset_index(drop = True)
    return df

# plotting a feature importance dataframe (horizontal barchart)
def var_imp_plot(imp_df, title):
    imp_df.columns = ['feature', 'feature_importance']
    sns.barplot(x = 'feature_importance', y = 'feature', data = imp_df, orient = 'h', color = 'royalblue') \
       .set_title(title, fontsize = 20)

""" Mengalisis pentingnya fitur (feature importance) dalam model machine learning"""

rfc_fimp = pd.Series(rf.feature_importances_, index = x.columns)
rfc_fimp.sort_values(ascending=False)

"""Nilai ini menggambarkan sejauh mana semua fitur tersebut berkontribusi dalam menjelaskan variabilitas dalam target SalePrice."""

plt.figure(figsize=(10,6))
rfc_fimp.nlargest(30).sort_values(ascending=False).plot(kind='barh')
round(rfc_fimp,4)*100
plt.title('Important Features',size=12)
plt.show()

"""Perbandingan kontribusi fitur fitur yang divisualisasikan dalam plot. Dapat dilihat bahwa fitur OverallQual sangat mendominasi dengan value lebih dari 0.5

## Linear Regression
"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sb
import plotly.express as px
plt.style.use('default')

model1 = LinearRegression()
model1.fit(x_train, y_train)

# evaluate model
display(model1.score(x_test, y_test))

#untuk memprediksi
ypred_lr = model1.predict(x_test)

ypred_lr = ypred_lr.reshape(-1,1)

r2_lr = r2_score(y_test,ypred_lr).round(2)
mse_lr = mean_squared_error(y_test, ypred_lr).round(2)
mae_lr = mean_absolute_error(y_test, ypred_lr).round(2)
rmse_lr = np.sqrt(mean_squared_error(y_test, ypred_lr)).round(2) #akar mse

print('R2 : ', r2_lr)
print('MSE : ', mse_lr)
print('MAE : ', mae_lr)
print('RMSE : ', rmse_lr)

"""## Ridge Reggresion"""

model2 = Ridge(alpha=0.01)
model2.fit(x_train, y_train)

# evaluate model
display(model2.score(x_test, y_test))

# use model to predict
ypred_rr = model2.predict(x_test)

ypred_rr = ypred_rr.reshape(-1,1)

r2_rr = r2_score(y_test,ypred_rr).round(2)
mse_rr = mean_squared_error(y_test, ypred_rr).round(2)
mae_rr = mean_absolute_error(y_test, ypred_rr).round(2)
rmse_rr = np.sqrt(mean_squared_error(y_test, ypred_rr)).round(2)

print('R2 : ', r2_rr)
print('MSE : ', mse_rr)
print('MAE : ', mae_rr)
print('RMSE : ', rmse_rr)

"""## Lasso Regression"""

model3 = Lasso(alpha=0.01)
model3.fit(x_train, y_train)

# evaluate model
display(model3.score(x_test, y_test))

# use model to predict
ypred_ls = model3.predict(x_test)

ypred_ls = ypred_ls.reshape(-1,1)

r2_ls = r2_score(y_test,ypred_ls).round(2)
mse_ls = mean_squared_error(y_test, ypred_ls).round(2)
mae_ls = mean_absolute_error(y_test, ypred_ls).round(2)
rmse_ls = np.sqrt(mean_squared_error(y_test, ypred_ls)).round(2)

print('R2 : ', r2_ls)
print('MSE : ', mse_ls)
print('MAE : ', mae_ls)
print('RMSE : ', rmse_ls)

"""## Decision Tree"""

model4 = DecisionTreeRegressor()
model4.fit(x_train, y_train)

# evaluate model
display(model4.score(x_test, y_test))

# use model to predict
ypred_dt = model4.predict(x_test)

ypred_dt = ypred_dt.reshape(-1,1)

r2_dt = r2_score(y_test,ypred_dt).round(2)
mse_dt = mean_squared_error(y_test, ypred_dt).round(2)
mae_dt = mean_absolute_error(y_test, ypred_dt).round(2)
rmse_dt = np.sqrt(mean_squared_error(y_test, ypred_dt)).round(2)

print('R2 : ', r2_dt)
print('MSE : ', mse_dt)
print('MAE : ', mae_dt)
print('RMSE : ', rmse_dt)

"""## Random Forest"""

# Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression

model4 = RandomForestRegressor(max_depth=2, random_state=0)
model4.fit(x_train, y_train)

# evaluate model
display(model4.score(x_test, y_test))

# use model to predict
ypred_rf = model4.predict(x_test)

ypred_rf = ypred_rf.reshape(-1,1)

r2_rf = r2_score(y_test,ypred_rf).round(2)
mse_rf = mean_squared_error(y_test, ypred_rf).round(2)
mae_rf = mean_absolute_error(y_test, ypred_rf).round(2)
rmse_rf = np.sqrt(mean_squared_error(y_test, ypred_rf)).round(2)

print('R2 : ', r2_rf)
print('MSE : ', mse_rf)
print('MAE : ', mae_rf)
print('RMSE : ', rmse_rf)

"""## Gradient Boosting"""

model6 = GradientBoostingRegressor()
model6.fit(x_train, y_train)

# evaluate model
display(model6.score(x_test, y_test))

# use model to predict
ypred_gb = model6.predict(x_test)

ypred_gb = ypred_gb.reshape(-1,1)

r2_gb = r2_score(y_test,ypred_gb).round(2)
mse_gb = mean_squared_error(y_test, ypred_gb).round(2)
mae_gb = mean_absolute_error(y_test, ypred_gb).round(2)
rmse_gb = np.sqrt(mean_squared_error(y_test, ypred_gb)).round(2)

print('R2 : ', r2_gb)
print('MSE : ', mse_gb)
print('MAE : ', mae_gb)
print('RMSE : ', rmse_gb)

"""#CROSS VALIDATION"""

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor

"""## Decision Tree Regression with CV

"""

model5 = DecisionTreeRegressor()
model5.fit(x_train, y_train)

# Melakukan cross-validation dengan 5-fold
scores = cross_val_score(model5, x_train, y_train, cv=5, scoring='r2')

# Menampilkan hasil cross-validation
print("Hasil cross-validation (R-squared):", scores)
print("Rata-rata R-squared:", scores.mean())

# Menggunakan model untuk memprediksi data pengujian
ypred_dt = model5.predict(x_test)

# Menghitung metrik evaluasi
r2_dt = r2_score(y_test, ypred_dt).round(2)
mse_dt = mean_squared_error(y_test, ypred_dt).round(2)
mae_dt = mean_absolute_error(y_test, ypred_dt).round(2)
rmse_dt = np.sqrt(mean_squared_error(y_test, ypred_dt)).round(2)

# Menampilkan metrik evaluasi
print('R2 : ', r2_dt)
print('MSE : ', mse_dt)
print('MAE : ', mae_dt)
print('RMSE : ', rmse_dt)

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
import pandas as pd

def learn_curve_plot(estimator, x, y, train_sizes, cv):
    train_sizes, train_scores, validation_scores = learning_curve(estimator, x, y, train_sizes=train_sizes, cv=cv, scoring='neg_mean_squared_error')
    train_scores_mean = -train_scores.mean(axis=1)
    validation_scores_mean = -validation_scores.mean(axis=1)

    plt.style.use('seaborn')
    plt.plot(train_sizes, train_scores_mean, label='Training MSE')
    plt.plot(train_sizes, validation_scores_mean, label='Validation MSE')
    plt.ylabel('Mean Squared Error', fontsize=14)
    plt.xlabel('Training set size', fontsize=14)
    plt.title('Learning Curve', fontsize=18, y=1.03)
    plt.legend()
    plt.show()

# Buat objek DecisionTreeRegressor dengan parameter yang sesuai
model_decision_tree = DecisionTreeRegressor()

# Panggil fungsi learn_curve_plot dengan model_decision_tree sebagai estimator
learn_curve_plot(estimator=model_decision_tree, x=x, y=y, train_sizes=[1, 20, 50, 100, 300, 500, 614], cv=5)

"""Pada learning curve decision tree ini model dapat dikatakan underfitting karena memiliki gap yang cukup besar.

## Random Forest with CV
"""

model5 = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42)
model5.fit(x_train, y_train)

# cross-validation dengan 5-fold
scores = cross_val_score(model5, x_train, y_train, cv=5, scoring='r2')

print("Hasil cross-validation (R-squared):", scores)
print("Rata-rata R-squared:", scores.mean())

ypred_rf = model5.predict(x_test)

r2_rf = r2_score(y_test, ypred_rf).round(2)
mse_rf = mean_squared_error(y_test, ypred_rf).round(2)
mae_rf = mean_absolute_error(y_test, ypred_rf).round(2)
rmse_rf = np.sqrt(mean_squared_error(y_test, ypred_rf)).round(2)

print('R2 : ', r2_rf)
print('MSE : ', mse_rf)
print('MAE : ', mae_rf)
print('RMSE : ', rmse_rf)

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
import pandas as pd

def learn_curve_plot(estimator, x, y, train_sizes, cv):
    train_sizes, train_scores, validation_scores = learning_curve(estimator, x, y, train_sizes=train_sizes, cv=cv, scoring='neg_mean_squared_error')
    train_scores_mean = -train_scores.mean(axis=1)
    validation_scores_mean = -validation_scores.mean(axis=1)

    plt.style.use('seaborn')
    plt.plot(train_sizes, train_scores_mean, label='Training MSE')
    plt.plot(train_sizes, validation_scores_mean, label='Validation MSE')
    plt.ylabel('Mean Squared Error', fontsize=14)
    plt.xlabel('Training set size', fontsize=14)
    plt.title('Learning Curve', fontsize=18, y=1.03)
    plt.legend()
    plt.show()

# Buat objek DecisionTreeRegressor dengan parameter yang sesuai
model_decision_tree = RandomForestRegressor()

# Panggil fungsi learn_curve_plot dengan model_decision_tree sebagai estimator
learn_curve_plot(estimator=model_decision_tree, x=x, y=y, train_sizes=[1, 20, 50, 100, 300, 500, 614], cv=5)

"""Pada learning curve random forest ini, gap yang dimiliki awalnya cukup jauh, namun semakin ke kanan garis antara validation dan dan training kemungkinan akan beririsan/bersinggungan. Namun, model ini juga masih dikatakan underfitting.

## Model Comparison
"""

#buat dataframe untuk compare beberapa model yang telah dibangun

data = np.array([['', 'R2', 'MSE', 'MAE', 'RMSE'],
                ['Linear Regression', r2_lr, mse_lr, mae_lr, rmse_lr],
                ['Ridge Regression', r2_rr, mse_rr, mae_rr, rmse_rr],
                ['Lasso Regression', r2_ls, mse_ls, mae_ls, rmse_ls],
                ['DTR', r2_dt, mse_dt, mae_dt, rmse_dt],
                ['RFR', r2_rf, mse_rf, mae_rf, rmse_rf],
                ['GB', r2_gb, mse_gb, mae_gb, rmse_gb]])

data

table = pd.DataFrame(data = data[1:, 1:],
                     index = data[1:,0],
                     columns = data[0,1:])
table

"""Dari hasil evaluasi model, model Gradient Boosting (GB) menunjukkan performa terbaik dengan nilai R2 tertinggi (0.84) dan tingkat kesalahan prediksi yang rendah, diukur dengan MSE (540304038.65) dan MAE (16848.78). Hal ini mengindikasikan kemampuan GB dalam menjelaskan variabilitas data target dan menghasilkan prediksi yang akurat. Model Random Forest Regression (RFR) juga memiliki performa yang baik dengan R2 sekitar 0.82 dan MSE sekitar 597072887.97. Model-model linear, seperti Linear Regression, Ridge Regression, dan Lasso Regression, memberikan hasil yang sebanding (R2 sekitar 0.81), tetapi memiliki tingkat kesalahan prediksi yang sedikit lebih tinggi. Model Decision Tree Regression (DTR) memiliki performa paling rendah dengan R2 sekitar 0.63 dan RMSE tertinggi (35002.56).

# HYPERPARAMETER TUNING
"""

from sklearn.model_selection import GridSearchCV
# Create the parameter grid based on the results of random search
param_grid = {
    'bootstrap': [True],
    'max_depth': [80, 90, 100, 110],
    'max_features': [2, 3],
    'min_samples_leaf': [3, 4, 5],
    'min_samples_split': [8, 10, 12],
    'n_estimators': [100, 200, 300, 1000]
}
# Create a based model
gb = GradientBoostingRegressor()

# Instantiate the grid search model
grid_search = GridSearchCV(estimator = rf, param_grid = param_grid,
                          cv = 5, n_jobs = -1, verbose = 2)

# Fit the grid search to the data
grid_search.fit(x_train, y_train)

grid_search.best_params_

best_grid = grid_search.best_estimator_
grid_accuracy = display(best_grid.score(x_test, y_test))
grid_accuracy

"""# LEARNING CURVE"""

from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
import pandas as pd

def learn_curve_plot(estimator, x, y, train_sizes, cv):
    train_sizes, train_scores, validation_scores = learning_curve(estimator, x, y, train_sizes=[1, 20, 50, 100, 300, 500, 614], cv=cv)
    train_scores_mean = -train_scores.mean(axis=1)
    validation_scores_mean = -validation_scores.mean(axis=1)

    # Print the mean training and validation scores
    print('Mean training scores\n', pd.Series(train_scores_mean, index=train_sizes))
    print('\n' + '-'*20) # Separator
    print('\nMean validation scores\n', pd.Series(validation_scores_mean, index=train_sizes))

    plt.style.use('seaborn')
    plt.plot(train_sizes, train_scores_mean, label='Training MSE')
    plt.plot(train_sizes, validation_scores_mean, label='Validation MSE')
    plt.ylabel('Mean Squared Error', fontsize=14)
    plt.xlabel('Training set size', fontsize=14)
    plt.title('Learning Curve', fontsize=18, y=1.03)
    plt.legend()
    plt.show()

# Panggil fungsi learn_curve_plot dengan model1 sebagai estimator
learn_curve_plot(estimator=model6, x=x, y=y, train_sizes=[1, 20, 50, 100, 300, 500, 614], cv=5)

"""Tampak gap pada learning curve diatas semakin ke kanan semakin mengecil, memungkinkan garis training akan bersinggungan dengan validation MSE, ini menunjukkan bahwa model akan semakin baik dalam memodelkan data. Kinerja model menjadi lebih stabil dan mungkin akan mencapai tingkat optimal

Sumber Referensi:
- Sesi Mentoring
- Live Session
- Modul Startup Campus
- https://www.w3schools.com/statistics/statistics_quartiles_and_percentiles.php
- https://www.dicoding.com/academies/615/tutorials/33103
"""